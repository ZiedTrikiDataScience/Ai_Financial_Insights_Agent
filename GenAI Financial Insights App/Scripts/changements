import os
from dotenv import load_dotenv
import streamlit as st
import requests
import pandas as pd
import tempfile
import base64
from loguru import logger
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import numpy as np
import matplotlib.pyplot as plt
from openai import OpenAI
from pathlib import Path
import textwrap
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.units import inch
import re
import time

# Load environment variables from .env
load_dotenv()
router_api_key = os.getenv("OPENROUTER_API_KEY")
guardian_api_key = os.getenv("GUARDIAN_API_KEY")
if not router_api_key:
    raise ValueError("Missing OPENROUTER_API_KEY in .env file.")
if not guardian_api_key:
    raise ValueError("Missing GUARDIAN_API_KEY in .env file.")

client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=router_api_key)

st.set_page_config(page_title="Investor Thematic Report Generator", layout="wide", initial_sidebar_state="expanded")
st.markdown("""<style>h2 { color: black !important; font-weight: bold !important; }</style>""", unsafe_allow_html=True)
st.title("\U0001F4CA Investor Thematic Report Generator")
st.markdown("Generate polished, investor-grade PDF reports from financial news summaries.")

image_path = Path(r"C:\Users\STAM\Desktop\GenAI Financial Insights App\images\fin_insights.png")
if image_path.exists():
    st.image(str(image_path), caption="GenAI Financial Insights", width=500)
else:
    st.warning("Banner image not found at:\n" + str(image_path))

st.sidebar.header("Settings")
from_date = st.sidebar.date_input("From Date", pd.to_datetime("2023-01-01"))
to_date = st.sidebar.date_input("To Date", pd.to_datetime("today"))


)

st.sidebar.markdown("---")
st.sidebar.markdown("**API keys loaded from .env**")

def fetch_articles_paginated(from_date, to_date, target_count):
    """Fetch articles with pagination to get more than 50 articles"""
    all_articles = []
    page = 1
    page_size = 50  # Maximum allowed by Guardian API
    
    while len(all_articles) < target_count:
        remaining = target_count - len(all_articles)
        current_page_size = min(page_size, remaining)
        
        endpoint = "https://content.guardianapis.com/search"
        params = {
            "q": "financial news",
            "section": "business",
            "from-date": from_date.isoformat(),
            "to-date": to_date.isoformat(),
            "show-fields": "bodyText,webPublicationDate",
            "page-size": 50,
            "page": page,
            "api-key": guardian_api_key
        }
        
        try:
            resp = requests.get(endpoint, params=params)
            resp.raise_for_status()
            data = resp.json()
            
            if "response" not in data or "results" not in data["response"]:
                break
                
            articles = data["response"]["results"]
            if not articles:  # No more articles
                break
            
            # Filter articles with valid content
            valid_articles = [a for a in articles if a.get("fields", {}).get("bodyText")]
            all_articles.extend(valid_articles)
            
            page += 1
            time.sleep(0.1)  # Small delay to be respectful to API
            
        except Exception as e:
            st.error(f"Error fetching page {page}: {e}")
            break
    
    return all_articles[:target_count]

def summarize_with_progress(articles_content):
    """Summarize articles with progress tracking"""
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn", framework="pt")
    summaries = []
    
    # Create progress bar
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, content in enumerate(articles_content):
        try:
            # Truncate content if too long
            content_truncated = content[:4000] if len(content) > 4000 else content
            
            if len(content_truncated.strip()) < 50:
                summaries.append("Content too short to summarize")
                continue
                
            summary = summarizer(content_truncated, max_length=150, min_length=40, do_sample=False)[0]["summary_text"]
            summaries.append(summary)
            
        except Exception as e:
            logger.warning(f"Error summarizing article {i}: {e}")
            summaries.append("Error during summarization")
        
        # Update progress
        progress = (i + 1) / len(articles_content)
        progress_bar.progress(progress)
        status_text.text(f"Summarizing: {i + 1}/{len(articles_content)} articles")
        
        # Small delay to prevent overwhelming the system
        if i % 10 == 0:
            time.sleep(0.1)
    
    progress_bar.empty()
    status_text.empty()
    
    return summaries

def find_best_k(embeddings, k_min=2, k_max=6):
    """Find optimal number of clusters, adjusted for larger datasets"""
    n_samples = len(embeddings)
    # Adjust k_max based on number of samples
    k_max = min(k_max, max(2, n_samples // 15))  # At least 15 articles per cluster
    
    if k_max <= k_min:
        return k_min, None
    
    scores = []
    k_values = list(range(k_min, k_max + 1))
    
    for k in k_values:
        try:
            kmeans = KMeans(n_clusters=k, random_state=42).fit(embeddings)
            score = silhouette_score(embeddings, kmeans.labels_)
            scores.append(score)
        except:
            scores.append(0)
    
    best_k = k_values[np.argmax(scores)]

    fig, ax = plt.subplots()
    ax.plot(k_values, scores, marker='o')
    ax.set_xlabel("Number of Clusters (k)")
    ax.set_ylabel("Silhouette Score")
    ax.set_title("Elbow Method - Silhouette Score")
    elbow_plot_path = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
    fig.savefig(elbow_plot_path.name)
    plt.close(fig)
    return best_k, elbow_plot_path.name

def generate_theme_names_batch(df, best_k):
    """Generate theme names with better error handling"""
    theme_names = {}
    
    for tid in sorted(df["theme_id"].unique()):
        cluster_summaries = df[df["theme_id"] == tid]["summary"].tolist()
        # Limit number of summaries to avoid token limits
        sample_summaries = cluster_summaries[:8]  # Use first 8 summaries
        joined = "\n\n".join(sample_summaries)
        
        prompt = f"""You are a financial analyst. Given these summaries, suggest a concise (3–5 word) label capturing their theme.
Return only the theme name as plain text. Do not include any quotes, symbols, or formatting like LaTeX or Markdown.

Summaries:
{joined[:3000]}
"""
        
        try:
            res = client.chat.completions.create(
                model="deepseek/deepseek-r1-zero:free",
                messages=[{"role": "user", "content": prompt}],
                extra_headers={"HTTP-Referer": "https://openrouter.ai", "X-Title": "Theme Label"}
            )
            raw_output = res.choices[0].message.content.strip()
            clean_output = re.sub(r'\\boxed\{["\']?(.*?)["\']?\}', r'\1', raw_output)
            theme_names[tid] = clean_output.strip()
            
        except Exception as e:
            logger.warning(f"Error generating theme name for cluster {tid}: {e}")
            theme_names[tid] = f"Financial Theme {tid + 1}"
        
        time.sleep(0.5)  # Delay between API calls
    
    return theme_names

def generate_insight(texts, tid, theme_name):
    """Generate insights with truncation to avoid token limits"""
    # Truncate texts to avoid token limits
    texts_truncated = texts[:6000] if len(texts) > 6000 else texts
    
    prompt = f"""
Generate a report section titled '{theme_name}'.

Based on these summaries:
"{texts_truncated}"

Write a structured analysis with the following sections:

**Executive Summary** : where you summarize the theme from its articles in 3/4 lines

**Key Trends** 

**Implications and Insights for Investors** : where you give valuable insights and advices for investors based on the happenings so that they can take better financial decisions.

Each section should be clearly labeled.
"""
    
    try:
        r = client.chat.completions.create(
            model="deepseek/deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            extra_headers={"HTTP-Referer": "https://openrouter.ai", "X-Title": "Insight Gen"}
        )
        return r.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Error generating insight for theme {tid}: {e}")
        return f"**{theme_name}**\n\nError generating content for this theme."

if st.button("\U0001F680 Run Full Pipeline", type='primary', use_container_width=True):
    st.subheader("Step 1: Scrape Financial News")
    st.info(f"Fetching {num_articles} articles from The Guardian API...")
    
    # Fetch articles with pagination
    articles = fetch_articles_paginated(from_date, to_date, num_articles)
    
    if not articles:
        st.error("No articles found. Please adjust your search criteria.")
        st.stop()
    
    rows = [{
        "publicationDate": pd.to_datetime(a["webPublicationDate"]).tz_localize(None),
        "article_content": a["fields"]["bodyText"]
    } for a in articles]
    df = pd.DataFrame(rows).sort_values("publicationDate", ascending=False)
    
    st.success(f"Scraped {len(df)} articles")
    st.dataframe(df[["publicationDate"]].head(5))

    st.subheader("Step 2: Summarize Articles")
    st.info("Summarizing content with BART...")
    
    # Summarize with progress tracking
    summaries = summarize_with_progress(df["article_content"].tolist())
    df["summary"] = summaries
    
    # Filter out failed summaries
    df = df[df["summary"] != "Error during summarization"].reset_index(drop=True)
    
    st.success(f"Articles summarized successfully ({len(df)} valid summaries)")
    st.dataframe(df[["summary"]].head(3))

    st.subheader("Step 3: Embed & Determine Optimal Number of Clusters")
    st.info("Generating embeddings...")
    
    embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    embeddings = embedder.encode(df["summary"].tolist(), convert_to_numpy=True)
    
    best_k, elbow_plot = find_best_k(embeddings)
    
    if elbow_plot:
        st.image(elbow_plot, caption=f"Optimal number of clusters: {best_k}", use_column_width=True)
    
    st.write("### \U0001F913 The best number of clusters is : ", best_k)

    st.subheader("Step 3.1: Clustering Summaries")
    df["theme_id"] = KMeans(n_clusters=best_k, random_state=42).fit_predict(embeddings)
    st.success(f"Generated {best_k} clusters")

    st.subheader("Step 3.2: Generate Human-Readable Theme Names")
    
    # Generate theme names with progress
    theme_names = generate_theme_names_batch(df, best_k)
    
    st.write("### \U0001F9E0 Interpreted Themes:")
    for tid, label in theme_names.items():
        article_count = len(df[df["theme_id"] == tid])
        st.markdown(f"**Theme {tid + 1}:** {label} ({article_count} articles)")

    st.subheader("Step 4: Generate PDF Report")
    st.info("Formatting report sections into PDF using ReportLab...")

    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
    c = canvas.Canvas(tmp.name, pagesize=letter)
    width, height = letter
    c.setFont("Helvetica-Bold", 16)
    c.drawCentredString(width / 2, height - 50, "Investor-Grade Thematic Report")
    c.setFont("Helvetica", 12)
    c.drawCentredString(width / 2, height - 70, f"Analysis of {len(df)} Financial Articles")
    y = height - 100

    # Progress tracking for PDF generation
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    themes = sorted(df["theme_id"].unique())
    
    for i, tid in enumerate(themes):
        status_text.text(f"Generating PDF section {i+1}/{len(themes)}: {theme_names.get(tid, f'Theme {tid + 1}')}")
        
        summaries = df[df["theme_id"] == tid]["summary"].tolist()
        content = generate_insight("\n\n".join(summaries), tid, theme_names.get(tid, f"Theme {tid + 1}"))
        theme_title = theme_names.get(tid, f"Theme {tid + 1}")
        
        # Add theme title
        if y < 100:
            c.showPage()
            y = height - 50
            
        c.setFont("Helvetica-Bold", 14)
        c.drawString(40, y, theme_title)
        y -= 20
        
        # Add content
        c.setFont("Helvetica", 11)
        for line in content.split("\n"):
            for subline in textwrap.wrap(line, 95):
                if y < 50:
                    c.showPage()
                    c.setFont("Helvetica", 11)
                    y = height - 50
                c.drawString(40, y, subline)
                y -= 14
            y -= 8
        y -= 16
        
        # Update progress
        progress_bar.progress((i + 1) / len(themes))
        
        # Small delay to prevent overwhelming the system
        time.sleep(0.1)

    c.setFont("Helvetica-Oblique", 10)
    c.drawCentredString(width / 2, 30, "Created by: Zeineb Moalla, Financial Data Science Master Student")
    c.save()

    progress_bar.empty()
    status_text.empty()

    with open(tmp.name, "rb") as f:
        b64 = base64.b64encode(f.read()).decode()
        st.markdown(f'<a href="data:application/pdf;base64,{b64}" download="report_{num_articles}_articles.pdf">⬇️ Download Report ({num_articles} articles)</a>', unsafe_allow_html=True)

    st.success("Pipeline completed successfully!")

st.markdown("---")
st.caption("© 2025 Zeineb Moalla | Financial Data Science Master Student")